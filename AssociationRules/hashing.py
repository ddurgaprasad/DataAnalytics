# -*- coding: utf-8 -*-
"""TransactionReduction_Hashing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IQLQ248zf6D5RT6BH_kvfKkQn9fDpXVV
"""

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from itertools import chain
from collections import OrderedDict
from itertools import combinations

corpus =['I1,I2,I5',
'I2,I4',
'I2,I3',
'I1,I2,I4',
'I1,I3',
'I2,I3',
'I1,I3',
'I1,I2,I3,I5',
'I1,I2,I3']

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names())
index_col=['T'+str(i+1)  for i in range(len(corpus))]
print(index_col)
print(X.toarray())
print(index_col)
print(X.toarray())

columns=vectorizer.get_feature_names()
columns

C={}
L={}
candidates=None
min_sup=3
def getItemCountTable(my_table,min_sup,candidates_temp):
  if candidates_temp is None:    
    df_L=my_table.loc[:, (my_table .sum(axis=0) >= min_sup)]      #Sum of columns is greater than minimum support
    df_L=df_L[df_L.sum(axis=1) >= min_sup]                        #Sum of rows is greater than minimum support
    return df_L
  else:
    rows_singleton=my_table.index.to_list() #Get min support rows
    cols_singleton=C[1].columns.to_list()   # Get min support singleton columns
    #print('rows_singleton ' ,rows_singleton)
    #print('cols_singleton ' ,cols_singleton)

    new_df=C[1][cols_singleton].copy()
    new_df=new_df[new_df.index.isin(my_table.index)]
    #print('new_df')
    #print(new_df)
    for itemset in candidates_temp:
      combination_string = ",".join(itemset)
      itemset_as_list=[]
      for item in itemset:
        itemset_as_list.append(item)
      #print(itemset_as_list)
      new_df[combination_string] = new_df[itemset_as_list].all(axis=1)
    #print('Before dropping singleton columns')
    #print(new_df)
    new_df.drop(cols_singleton,axis=1,inplace=True)

    #print('After dropping singleton columns')
    #print(new_df)

    s=new_df.reset_index().melt('index')
    df_ctab=pd.crosstab(index=s['index'],columns=s.variable,values=s.value,aggfunc='sum',margins=True)
    
    df_freq=pd.DataFrame()
    df_freq['itemset']=df_ctab.columns.to_list()[:-1]
    df_freq['freq']=df_ctab.iloc[-1][:-1].to_list()


  return new_df,df_freq

# Python3 program to remove duplicate 
# tuples from list of tuples 

def removeDuplicates(lst): 	
	return [t for t in (set(tuple(i) for i in lst))] 
	
def getCandidateSets(arr,tuple_size,iteration):   
  temp_itemtuple_list=[]
  temp=list(combinations(arr,2))  
  for item in temp:
    mylist=(item[0]+',' +item[1]).split(',')
    t=sorted(list(dict.fromkeys(mylist))) #Order the item list lexigraphically
    if len(t) == iteration+1:
      valid_tuple=t
      temp_itemtuple_list.append(valid_tuple)

  temp_itemtuple_list=removeDuplicates(temp_itemtuple_list)   #Remove duplicate tuples from the list 
  return temp_itemtuple_list

k=1
C[k]=pd.DataFrame(data=X.toarray(), index=index_col ,columns=columns)
print(C[k])

s=C[k].reset_index().melt('index')
df_ctab=pd.crosstab(index=s['index'],columns=s.variable,values=s.value,aggfunc='sum',margins=True)

df_freq=pd.DataFrame()
df_freq['itemset']=df_ctab.columns.to_list()[:-1]
df_freq['freq']=df_ctab.iloc[-1][:-1].to_list()
df_freq

"""**Hashing function**

H(x,y)= ((Order of first)*10 + (Order of second)) mod 7
"""

candidates=getCandidateSets(C[k].columns.to_list(),2,k)
L[k],df_ctab=getItemCountTable(C[k],min_sup,candidates)
print(L[k]) 
print(df_ctab)
k=k+1

order_of_items={}
for idx, val in enumerate(columns):
    #print(idx+1, val)
    order_of_items[val]=idx+1
order_of_items

df_ctab['item1'], df_ctab['item2'] = df_ctab['itemset'].str.split(',', 1).str
df_ctab['order_of_first']   =  df_ctab['item1'].map(order_of_items) 
df_ctab['order_of_second']   = df_ctab['item2'].map(order_of_items)

df_ctab['bucket'] =  np.mod(((df_ctab['order_of_first'])*10 + (df_ctab['order_of_second'])) , 7)

df_ctab = df_ctab[['itemset','freq','bucket']]
df_ctab

ds_bucket_count=df_ctab.groupby(['bucket','itemset'], as_index=False)['freq'].sum()
ds_bucket_count

ds_bucket_count[ds_bucket_count['freq'] >= min_sup]

buckets_with_minsup=ds_bucket_count[ds_bucket_count['freq'] >= min_sup]['bucket'].to_list()
buckets_with_minsup

df_freqitems= df_ctab[df_ctab.index.isin(buckets_with_minsup)]
df_freqitems

freq_itemsets=df_freqitems['itemset'].to_list()
freq_itemsets