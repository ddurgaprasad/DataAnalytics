# -*- coding: utf-8 -*-
"""WineQualityEnsemble.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X74aIUF0uwLyehl_E2OK9UqpqNiVbxj2

**Installing Libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""**Importing the Dataset**"""

data = pd.read_csv('train.csv' ,header=None)
data.head()

data.columns=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
       'pH', 'sulphates', 'alcohol', 'quality']

data.describe()

data.info()

data.isnull().sum()

"""**Class imbalance**"""

data['quality'].value_counts()

"""**Data Visualization**"""

sns.countplot(data['quality'])

"""**Bivariate Analysis**"""

# checking the variation of fixed acidity in the different qualities of wine

plt.scatter(data['quality'], data['fixed acidity'], color = 'green')
plt.title('relation of fixed acidity with wine')
plt.xlabel('quality')
plt.ylabel('fixed acidity')
plt.legend()
plt.show()

# checking the variation of fixed acidity in the different qualities of wine

plt.bar(data['quality'], data['alcohol'], color = 'maroon')
plt.title('relation of alcohol with wine')
plt.xlabel('quality')
plt.ylabel('alcohol')
plt.legend()
plt.show()

# Composition of citric acid go higher as we go higher in the quality of the wine

import seaborn as sns

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'citric acid', data = data)

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'residual sugar', data = data)

#Composition of chloride also go down as we go higher in the quality of the wine

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'chlorides', data = data)

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'free sulfur dioxide', data = data)

#Sulphates level goes higher with the quality of wine
fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'sulphates', data = data)

#Sulphates level goes higher with the quality of wine

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'volatile acidity', data = data)

"""As we can see that like the above two items do not have very strong relation to the dependent variable we have to showcase a correlation plot to check which of the items are more related to the dependent variable and which items are less related to the dependent variables."""

f, ax = plt.subplots(figsize=(10, 8))
corr = data.corr()
sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax)

"""From the above correlation plot for the given dataset for wine quality prediction, we can easily see which items are related strongly with each other and which items are related weekly with each other.
For Example, 
# The strongly correlated items are :

1.fixed acidity and citric acid.
2.free sulphur dioxide and total sulphor dioxide.
3.fixed acidity and density.
4. alcohol and quality.

so, from above points there is a clear inference that alcohol is the most important characteristic to determine the quality of wine.

# The weekly correlated items are :

1.citric acid and volatile acidity.
2.fixed acidity and ph.
3.density and alcohol.

These are some relations which do not depend on each other at all.
"""

sns.pairplot(data)



"""**Data pre-processing**


1.   Loading
2.   Normalizaiton
1.   PCA
2.   Train and Test split
"""

X=data.drop(columns=['quality'])
y=data['quality'].values
print(X)
print(y)

# standard scaling 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)

from sklearn.decomposition import PCA
pca=PCA(n_components=9)
pca.fit(X)

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');

var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)
var

from sklearn.decomposition import PCA

pca = PCA()
X_pca = pca.fit_transform(X)

from sklearn.model_selection import train_test_split
def TestTrainSplit(X,y):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 2489)

  return X_train, X_test, y_train, y_test

"""# **SMOTE**"""

!pip install imbalanced-learn

# check version number
import imblearn
print(imblearn.__version__)

from imblearn.over_sampling import SMOTE
# transform the dataset
oversample=SMOTE(kind='regular',k_neighbors=2)
X_SMOTE, y_SMOTE = oversample.fit_resample(X, y)

from collections import Counter
Counter(y_SMOTE)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.model_selection import GridSearchCV, cross_val_score

from sklearn.ensemble import RandomForestClassifier

def RFC():
  X_train, X_test, y_train, y_test = train_test_split(X_SMOTE, y_SMOTE, test_size = 0.25, random_state = 2489)

  # creating the model
  model = RandomForestClassifier(n_estimators = 200)

  # feeding the training set into the model
  model.fit(X_train, y_train)

  # predicting the results for the test set
  y_pred = model.predict(X_test)

  # calculating the training and testing accuracies
  print("Training accuracy :", model.score(X_train, y_train))
  print("Testing accuracy :", model.score(X_test, y_test))

  # classification report
  print(classification_report(y_test, y_pred))

  # confusion matrix
  print(confusion_matrix(y_test, y_pred))

  model_eval = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 10)
  print('mean ', model_eval.mean())


RFC()

import xgboost as xgb

def XGB():
  X_train, X_test, y_train, y_test = train_test_split(X_SMOTE, y_SMOTE, test_size = 0.25, random_state = 2489)

  # creating the model
  model = xgb.XGBClassifier(random_state=2489)

  # feeding the training set into the model
  model.fit(X_train, y_train)

  # predicting the results for the test set
  y_pred = model.predict(X_test)

  # calculating the training and testing accuracies
  print("Training accuracy :", model.score(X_train, y_train))
  print("Testing accuracy :", model.score(X_test, y_test))

  # classification report
  print(classification_report(y_test, y_pred))

  # confusion matrix
  print(confusion_matrix(y_test, y_pred))

  model_eval = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 10)
  print('mean ', model_eval.mean())


XGB()